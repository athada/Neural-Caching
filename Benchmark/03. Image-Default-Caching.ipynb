{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7393823-9084-4f53-ba91-d80d3578059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from tensorflow.keras.applications import NASNetMobile\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696bd5b1-05c2-4034-8537-2af58081899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:30:35.904888: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-02-05 12:30:35.904904: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-02-05 12:30:35.904909: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2025-02-05 12:30:35.904922: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-05 12:30:35.904931: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Ensure TensorFlow only uses a limited amount of GPU memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Define batch sizes and number of runs\n",
    "BATCH_SIZES = [1, 4, 8, 16, 32, 64]\n",
    "NUM_RUNS = 5  # Runs per batch size\n",
    "\n",
    "# Load NASNet model\n",
    "base_model = NASNetMobile(weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Create a model that outputs intermediate layer results\n",
    "layer_outputs = [layer.output for layer in base_model.layers]\n",
    "model = Model(inputs=base_model.input, outputs=layer_outputs)\n",
    "\n",
    "# Precompile TensorFlow functions for performance\n",
    "@tf.function\n",
    "def run_inference(input_tensor):\n",
    "    return model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f0fe96-1c55-4633-a6a0-93c92f844add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-100 dataset\n",
    "def load_cifar100(batch_size):\n",
    "    dataset = tfds.load(\"cifar100\", split=\"train\", as_supervised=True)\n",
    "    dataset = dataset.map(lambda x, y: (tf.image.resize(x, [224, 224]), y))  # Resize to 224x224\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf2fae3-8492-4725-bf65-259d1b5e9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure inference time and memory\n",
    "def benchmark_model(device, batch_size, dataset_iter):\n",
    "    print(f\"\\nRunning on {device} with batch size {batch_size}\")\n",
    "    results = []\n",
    "\n",
    "    for run in range(NUM_RUNS):\n",
    "        # Load Dataset\n",
    "        try:\n",
    "            input_tensor, _ = next(dataset_iter)  # Get real ImageNet images\n",
    "        except StopIteration:\n",
    "            dataset_iter = iter(dataset)  # Restart dataset if exhausted\n",
    "            input_tensor, _ = next(dataset_iter)\n",
    "        \n",
    "        with tf.device(device):\n",
    "            tensor_sizes = {}\n",
    "\n",
    "            # Warm-up to avoid cold start latency\n",
    "            _ = run_inference(input_tensor)\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            # Measure Layer wise tensor size\n",
    "            output = run_inference(input_tensor)\n",
    "            \n",
    "            # Measure per-layer execution time\n",
    "            for idx, layer in enumerate(model.layers):\n",
    "                # Calculate tensor size in MB (float32 = 4 bytes)\n",
    "                tensor_size = np.prod(output[idx].shape) * 4 / (1024 ** 2)\n",
    "                tensor_sizes[layer.name] = tensor_size\n",
    "            \n",
    "            total_time = time.perf_counter() - start_time\n",
    "    \n",
    "            results.append({\n",
    "                \"device\": device,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"total_time\": total_time\n",
    "            })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce971083-6c41-428c-8b7f-4709cd3eb672",
   "metadata": {},
   "source": [
    "## Experiment Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dee6e6d-e078-48de-98c9-aeaa75b53d8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:30:38.377764: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:30:38.377774: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /CPU:0 with batch size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:30:39.925688: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-05 12:30:40.896447: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:30:40.962006: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:30:40.962480: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/CPU/1/plugins/profile/2025_02_05_12_30_40/C17586.xplane.pb\n",
      "2025-02-05 12:30:41.008910: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-05 12:30:41.010002: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:30:41.010008: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /CPU:0 with batch size 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:30:43.963011: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:30:44.043462: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:30:44.043618: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/CPU/4/plugins/profile/2025_02_05_12_30_44/C17586.xplane.pb\n",
      "2025-02-05 12:30:44.092295: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-05 12:30:44.100219: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:30:44.100225: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /CPU:0 with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:30:47.534369: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:30:47.613716: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:30:47.613861: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/CPU/8/plugins/profile/2025_02_05_12_30_47/C17586.xplane.pb\n",
      "2025-02-05 12:30:47.677668: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-05 12:30:47.683107: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:30:47.683125: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /CPU:0 with batch size 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:30:52.077541: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:30:52.165150: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:30:52.165360: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/CPU/16/plugins/profile/2025_02_05_12_30_52/C17586.xplane.pb\n",
      "2025-02-05 12:30:52.219125: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-05 12:30:52.231279: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:30:52.231289: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /CPU:0 with batch size 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:31:00.542660: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:31:00.634814: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:31:00.635031: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/CPU/32/plugins/profile/2025_02_05_12_31_00/C17586.xplane.pb\n",
      "2025-02-05 12:31:00.693805: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-05 12:31:00.699158: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:31:00.699165: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /CPU:0 with batch size 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:31:16.681332: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:31:16.850599: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:31:16.851178: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/CPU/64/plugins/profile/2025_02_05_12_31_16/C17586.xplane.pb\n",
      "2025-02-05 12:31:17.045595: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-05 12:31:17.061556: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:31:17.061566: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /GPU:0 with batch size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:31:23.129405: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:31:23.144214: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:31:23.144514: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/GPU/1/plugins/profile/2025_02_05_12_31_23/C17586.xplane.pb\n",
      "2025-02-05 12:31:23.187810: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-05 12:31:23.189978: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:31:23.189983: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /GPU:0 with batch size 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:31:34.797204: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:31:34.811694: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:31:34.813486: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/GPU/4/plugins/profile/2025_02_05_12_31_34/C17586.xplane.pb\n",
      "2025-02-05 12:31:34.857649: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-05 12:31:34.859032: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:31:34.859035: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /GPU:0 with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:31:44.067576: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:31:44.084170: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:31:44.085430: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/GPU/8/plugins/profile/2025_02_05_12_31_44/C17586.xplane.pb\n",
      "2025-02-05 12:31:44.127566: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-05 12:31:44.128922: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:31:44.128928: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /GPU:0 with batch size 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:32:10.121826: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:32:10.139398: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:32:10.140417: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/GPU/16/plugins/profile/2025_02_05_12_32_10/C17586.xplane.pb\n",
      "2025-02-05 12:32:10.184816: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-05 12:32:10.185845: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:32:10.185851: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /GPU:0 with batch size 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:32:34.239167: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:32:34.279092: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:32:34.280180: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/GPU/32/plugins/profile/2025_02_05_12_32_34/C17586.xplane.pb\n",
      "2025-02-05 12:32:34.359398: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-05 12:32:34.361635: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2025-02-05 12:32:34.361641: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running on /GPU:0 with batch size 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:33:26.284700: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2025-02-05 12:33:26.712308: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2025-02-05 12:33:26.723687: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/Noise/GPU/64/plugins/profile/2025_02_05_12_33_26/C17586.xplane.pb\n"
     ]
    }
   ],
   "source": [
    "# Run benchmarks for CPU and GPU (if available)\n",
    "devices = [\"/CPU:0\"]\n",
    "if gpus:\n",
    "    devices.append(\"/GPU:0\")\n",
    "\n",
    "final_results = []\n",
    "\n",
    "for device in devices:    \n",
    "    for batch_size in BATCH_SIZES:\n",
    "        dataset = load_cifar100(batch_size)\n",
    "        dataset_iter = iter(dataset)\n",
    "        try:\n",
    "            tf.profiler.experimental.start(f\"logs/Noise/{device[1:4]}/{batch_size}\")\n",
    "            final_results.extend(benchmark_model(device, batch_size, dataset_iter))\n",
    "        finally:\n",
    "            tf.profiler.experimental.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f683f918-bf38-476e-8105-f3412daa03a1",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89c2cdf-e266-49e0-ba22-1cd44079a2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/CPU:0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/CPU:0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/CPU:0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/CPU:0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/CPU:0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   device  batch_size  total_time\n",
       "0  /CPU:0           1    0.051766\n",
       "1  /CPU:0           1    0.050228\n",
       "2  /CPU:0           1    0.034767\n",
       "3  /CPU:0           1    0.046638\n",
       "4  /CPU:0           1    0.031859"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(final_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa9939fb-12b7-4db5-8ea2-b6b62549cddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">total_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device</th>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">/CPU:0</th>\n",
       "      <th>1</th>\n",
       "      <td>0.031859</td>\n",
       "      <td>0.043052</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.051766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073920</td>\n",
       "      <td>0.083248</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.097852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.103913</td>\n",
       "      <td>0.128101</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.149268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.192116</td>\n",
       "      <td>0.249096</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.296424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.658166</td>\n",
       "      <td>0.770774</td>\n",
       "      <td>0.008709</td>\n",
       "      <td>0.884986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.594943</td>\n",
       "      <td>1.723794</td>\n",
       "      <td>0.014607</td>\n",
       "      <td>1.869776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">/GPU:0</th>\n",
       "      <th>1</th>\n",
       "      <td>0.083156</td>\n",
       "      <td>0.102858</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.172389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.087358</td>\n",
       "      <td>0.165196</td>\n",
       "      <td>0.011208</td>\n",
       "      <td>0.294188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.085706</td>\n",
       "      <td>0.411656</td>\n",
       "      <td>0.511807</td>\n",
       "      <td>1.691380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.090237</td>\n",
       "      <td>0.166147</td>\n",
       "      <td>0.018041</td>\n",
       "      <td>0.402420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.181739</td>\n",
       "      <td>1.427878</td>\n",
       "      <td>4.699295</td>\n",
       "      <td>5.244562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.959132</td>\n",
       "      <td>1.302995</td>\n",
       "      <td>0.174002</td>\n",
       "      <td>1.977547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  total_time                              \n",
       "                         min      mean       var       max\n",
       "device batch_size                                         \n",
       "/CPU:0 1            0.031859  0.043052  0.000084  0.051766\n",
       "       4            0.073920  0.083248  0.000118  0.097852\n",
       "       8            0.103913  0.128101  0.000362  0.149268\n",
       "       16           0.192116  0.249096  0.001474  0.296424\n",
       "       32           0.658166  0.770774  0.008709  0.884986\n",
       "       64           1.594943  1.723794  0.014607  1.869776\n",
       "/GPU:0 1            0.083156  0.102858  0.001515  0.172389\n",
       "       4            0.087358  0.165196  0.011208  0.294188\n",
       "       8            0.085706  0.411656  0.511807  1.691380\n",
       "       16           0.090237  0.166147  0.018041  0.402420\n",
       "       32           0.181739  1.427878  4.699295  5.244562\n",
       "       64           0.959132  1.302995  0.174002  1.977547"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['device', 'batch_size']).agg({'total_time': ['min', 'mean', 'var', 'max']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769886b-f85f-489f-ad64-b263751231d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
